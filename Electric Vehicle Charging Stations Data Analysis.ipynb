{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark  Electric Vehicle Charging Stations Data Analysis Project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly practice some new skills with Spark DataFrame! You will be asked some basic questions about charging data, in this case, the data from Boulder County, Colorado in 2019.\n",
    "\n",
    "Now, just answer the questions and complete the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the forecasting_all.csv file to Answer and complete the  tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('forecasting').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the forecasting_all.csv File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('forecasting_all.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      " |-- 20: double (nullable = true)\n",
      " |-- 21: double (nullable = true)\n",
      " |-- 22: double (nullable = true)\n",
      " |-- 23: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.date(2019, 1, 1), 0=None, 1=None, 2=None, 3=None, 4=None, 5=None, 6=None, 7=None, 8=None, 9=None, 10=3.51554, 11=3.45152, 12=3.02768, 13=8.36806, 14=11.02968, 15=10.95409, 16=12.3612, 17=9.38039, 18=6.7866, 19=3.73263, 20=4.20168, 21=6.0024, 22=6.0024, 23=3.80152) \n",
      "\n",
      "Row(Date=datetime.date(2019, 1, 2), 0=None, 1=None, 2=None, 3=None, 4=None, 5=1.06072, 6=7.34284, 7=8.073, 8=10.75761, 9=20.7757, 10=16.75676, 11=8.69116, 12=5.4848, 13=6.49981, 14=10.23954, 15=12.408, 16=12.408, 17=9.48714, 18=5.9202, 19=2.81788, 20=6.2343, 21=1.643, 22=None, 23=None) \n",
      "\n",
      "Row(Date=datetime.date(2019, 1, 3), 0=None, 1=None, 2=None, 3=None, 4=None, 5=0.06969, 6=1.3938, 7=6.35407, 8=7.96855, 9=11.95356, 10=21.49379, 11=24.37158, 12=33.06133, 13=24.05072, 14=14.02036, 15=14.26897, 16=10.23278, 17=8.03844, 18=6.86675, 19=13.79666, 20=8.51358, 21=4.92492, 22=None, 23=None) \n",
      "\n",
      "Row(Date=datetime.date(2019, 1, 4), 0=None, 1=None, 2=None, 3=None, 4=None, 5=0.8983, 6=5.0061, 7=6.79905, 8=7.98048, 9=9.26498, 10=14.79712, 11=14.1755, 12=5.022, 13=15.61674, 14=14.595, 15=17.44139, 16=21.90835, 17=18.63963, 18=15.94409, 19=3.44714, 20=None, 21=5.149, 22=5.86986, 23=None) \n",
      "\n",
      "Row(Date=datetime.date(2019, 1, 5), 0=6.609, 1=0.55075, 2=None, 3=None, 4=None, 5=None, 6=None, 7=6.13928, 8=None, 9=11.43851, 10=11.8138, 11=6.58421, 12=11.94723, 13=11.82453, 14=4.93845, 15=12.97806, 16=7.81084, 17=7.75665, 18=7.96596, 19=8.37716, 20=6.609, 21=6.609, 22=6.609, 23=6.609) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in df.head(5):\n",
    "    print(line, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|                 0|                1|                 2|                 3|                 4|                 5|                 6|                7|                 8|                 9|                10|               11|                12|               13|                14|                15|                16|                17|                18|                19|                20|                21|                22|                23|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               142|              101|                67|                35|                24|               150|               248|              294|               340|               354|               359|              361|               362|              360|               362|               358|               356|               357|               358|               357|               341|               305|               220|               175|\n",
      "|   mean|4.5170309859154925|4.282740594059405|3.7364498507462685|3.5655185714285715|3.0286704166666674|2.0185154666666656| 5.007421250000002|7.723809557823131|11.776469147058824| 18.87222954802261|20.020810278551526|20.40731421052632|19.896154392265206|18.94714852777777|16.699892790055237|14.983233743016754|14.235585084269657|15.060788655462185|17.440008994413414|15.831880700280118|11.658516803519072|7.2377464262295135| 5.714811090909094| 5.278458628571429|\n",
      "| stddev|2.7834380150997737|2.459543151161948|2.2786106902801686|1.9639414565360243|2.2577876628094593|1.8632879601964014|3.4552900363042536|4.585131689913158| 7.313427216756226|10.406141670762624|10.716492428216352|9.713006479117809| 9.083186076417372|8.533750466464681|  8.21190182597137|7.7067720932151405| 7.511211629821958| 7.873242049268241| 8.921337266907633| 8.271668662727192| 6.860061420305861| 4.967531940366416|3.6754879234797366|2.9280369986641883|\n",
      "|    min|           0.06955|          0.14134|            0.3986|           0.09276|           0.07794|           0.04039|            0.1117|           0.1006|           0.05617|           0.30936|           0.52335|          0.22998|           1.34589|          0.66066|           0.90459|           0.05834|            0.2639|            0.6722|           0.93434|            0.4247|           0.16887|           0.04282|           0.09917|           0.16416|\n",
      "|    max|           12.8337|           11.733|          11.41083|            6.9738|            6.9738|          12.87506|          18.26867|         25.59671|          47.39603|          52.56014|          53.22409|         53.63536|           47.1654|         47.85992|          44.13196|          47.08999|          47.19205|          40.60264|          49.15453|          43.13348|          36.02123|          25.86018|           21.5154|          14.98011|\n",
      "+-------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pyspark.sql.types import (StructField, StringType,\n",
    "                               FloatType, StructType)\n",
    "\n",
    "data_schema = [StructField('summary', StringType(), True),\n",
    "               StructField('0', FloatType(), True),]\n",
    "\n",
    "final_struc = StructType(fields=data_schema)\n",
    "\n",
    "'''\n",
    "df = spark.read.csv('forecasting_all.csv', inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|summary|     0|     1|    2|    3|    4|     5|     6|     7|     8|     9|    10|    11|    12|    13|    14|    15|\n",
      "+-------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|  count|142.00|101.00|67.00|35.00|24.00|150.00|248.00|294.00|340.00|354.00|359.00|361.00|362.00|360.00|362.00|358.00|\n",
      "|   mean|  4.52|  4.28| 3.74| 3.57| 3.03|  2.02|  5.01|  7.72| 11.78| 18.87| 20.02| 20.41| 19.90| 18.95| 16.70| 14.98|\n",
      "| stddev|  2.78|  2.46| 2.28| 1.96| 2.26|  1.86|  3.46|  4.59|  7.31| 10.41| 10.72|  9.71|  9.08|  8.53|  8.21|  7.71|\n",
      "|    min|  0.07|  0.14| 0.40| 0.09| 0.08|  0.04|  0.11|  0.10|  0.06|  0.31|  0.52|  0.23|  1.35|  0.66|  0.90|  0.06|\n",
      "|    max| 12.83| 11.73|11.41| 6.97| 6.97| 12.88| 18.27| 25.60| 47.40| 52.56| 53.22| 53.64| 47.17| 47.86| 44.13| 47.09|\n",
      "+-------+------+------+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "summary = df.describe()\n",
    "summary.select(summary['summary'],\n",
    "                  format_number(summary['0'].cast('float'), 2).alias('0'),\n",
    "                  format_number(summary['1'].cast('float'), 2).alias('1'),\n",
    "                  format_number(summary['2'].cast('float'), 2).alias('2'),\n",
    "                  format_number(summary['3'].cast('float'), 2).alias('3'),\n",
    "                  format_number(summary['4'].cast('float'), 2).alias('4'),\n",
    "                  format_number(summary['5'].cast('float'), 2).alias('5'),\n",
    "               format_number(summary['6'].cast('float'), 2).alias('6'),\n",
    "               format_number(summary['7'].cast('float'), 2).alias('7'),\n",
    "               format_number(summary['8'].cast('float'), 2).alias('8'),\n",
    "               format_number(summary['9'].cast('float'), 2).alias('9'),\n",
    "               format_number(summary['10'].cast('float'), 2).alias('10'),\n",
    "               format_number(summary['11'].cast('float'), 2).alias('11'),\n",
    "               format_number(summary['12'].cast('float'), 2).alias('12'),\n",
    "               format_number(summary['13'].cast('float'), 2).alias('13'),\n",
    "               format_number(summary['14'].cast('float'), 2).alias('14'),\n",
    "               format_number(summary['15'].cast('float'), 2).alias('15')\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+------+------+------+------+------+\n",
      "|summary|    16|    17|    18|    19|    20|    21|    22|    23|\n",
      "+-------+------+------+------+------+------+------+------+------+\n",
      "|  count|356.00|357.00|358.00|357.00|341.00|305.00|220.00|175.00|\n",
      "|   mean| 14.24| 15.06| 17.44| 15.83| 11.66|  7.24|  5.71|  5.28|\n",
      "| stddev|  7.51|  7.87|  8.92|  8.27|  6.86|  4.97|  3.68|  2.93|\n",
      "|    min|  0.26|  0.67|  0.93|  0.42|  0.17|  0.04|  0.10|  0.16|\n",
      "|    max| 47.19| 40.60| 49.15| 43.13| 36.02| 25.86| 21.52| 14.98|\n",
      "+-------+------+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(summary['summary'],\n",
    "               format_number(summary['16'].cast('float'), 2).alias('16'),\n",
    "               format_number(summary['17'].cast('float'), 2).alias('17'),\n",
    "               format_number(summary['18'].cast('float'), 2).alias('18'),\n",
    "               format_number(summary['19'].cast('float'), 2).alias('19'),\n",
    "               format_number(summary['20'].cast('float'), 2).alias('20'),\n",
    "               format_number(summary['21'].cast('float'), 2).alias('21'),\n",
    "               format_number(summary['22'].cast('float'), 2).alias('22'),\n",
    "               format_number(summary['23'].cast('float'), 2).alias('23')\n",
    "               \n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Charge per Hour for the Year and show up to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+-----+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+\n",
      "|Date|     0|     1|     2|     3|    4|     5|       6|       7|       8|       9|      10|      11|      12|      13|      14|      15|      16|      17|      18|      19|      20|      21|      22|    23|\n",
      "+----+------+------+------+------+-----+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+\n",
      "|null|641.42|432.56|250.34|124.79|72.69|302.78|1,241.84|2,270.80|4,004.00|6,680.77|7,187.47|7,367.04|7,202.41|6,820.97|6,045.36|5,364.00|5,067.87|5,376.70|6,243.52|5,651.98|3,975.55|2,207.51|1,257.26|923.73|\n",
      "+----+------+------+------+------+-----+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Another way to write\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Convert each column to double type\n",
    "df_double = df.select([col(c).cast('double') for c in df.columns])\n",
    "\n",
    "# Sum each column\n",
    "sum_result = df_double.agg(*[sum(col).alias(col) for col in df_double.columns])\n",
    "\n",
    "# Limit the number of decimal places\n",
    "formatted_result = sum_result.select(\n",
    "    *[format_number(col(c), 2).alias(c) for c in sum_result.columns]\n",
    ")\n",
    "\n",
    "# Show results\n",
    "formatted_result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe with a column called AGV(0-3H) that is the total charging amount between 0:00 (midnight) and 3:00 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         AGV(0-3H)|\n",
      "+------------------+\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|           7.15975|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|           5.76576|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|               0.0|\n",
      "|          11.01324|\n",
      "|            0.3317|\n",
      "|            2.7804|\n",
      "|15.759820000000001|\n",
      "|           3.36629|\n",
      "|           9.54897|\n",
      "|           1.10902|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "df_hv = df.withColumn('AGV(0-3H)', df['0']+df['1']+df['2']).select(['AGV(0-3H)'])\n",
    "df_hv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What day had the Peak High of Charging amount in 18H ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.date(2019, 9, 13))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df['18'].desc()).select(['Date']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the max and min of Charging amount in 18H ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "| max(18)|min(18)|\n",
      "+--------+-------+\n",
      "|49.15453|    0.0|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(max('18'),min('18')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many days was the Charging amount lower than 3 kw in 18H?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['18'] < 3).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of the time was the High greater than 200 kw ?\n",
    "#### In other words, (Number of Days High>200)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_date = df.drop('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_sum = df.withColumn(\"sum24H\", df['0'] + df['1'] + df['2'] + df['3'] + df['4'] + df['5'] + df['6'] + df['7'] + df['8'] + df['9'] + df['10'] + df['11'] + df['12'] + df['13'] + df['14'] + df['15'] + df['16'] + df['17'] + df['18'] + df['19'] + df['20'] + df['21'] + df['22'] + df['23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            sum24H|\n",
      "+------------------+\n",
      "| 92.61538999999998|\n",
      "|         146.60046|\n",
      "|         211.37955|\n",
      "|182.55472999999998|\n",
      "|         143.17043|\n",
      "|176.25732999999997|\n",
      "|250.48192000000003|\n",
      "|180.68962000000002|\n",
      "|         325.21041|\n",
      "|         232.11298|\n",
      "|         213.00623|\n",
      "|         135.76594|\n",
      "|176.14228000000003|\n",
      "|         277.52525|\n",
      "| 280.0675999999999|\n",
      "|         302.19502|\n",
      "|246.56322000000003|\n",
      "|225.57250000000005|\n",
      "|187.45399999999998|\n",
      "|         150.90042|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_sum.select('sum24H').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.04407713498622"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_with_sum.filter('sum24H > 200').count() * 100/df_with_sum.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the max sum per month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|        max(dfsum)|\n",
      "+-----+------------------+\n",
      "|   12|424.69754000000006|\n",
      "|    1|404.81627000000003|\n",
      "|    6|373.87354000000005|\n",
      "|    3|376.42245999999994|\n",
      "|    5|373.89501000000007|\n",
      "|    9|         340.04068|\n",
      "|    4|325.02076999999997|\n",
      "|    8|         385.32561|\n",
      "|    7|410.82541000000003|\n",
      "|   10|387.20394000000005|\n",
      "|   11| 507.3198999999999|\n",
      "|    2|409.21792999999997|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, hour,\n",
    "                                   dayofyear, month,\n",
    "                                   year, weekofyear,\n",
    "                                  format_number, date_format)\n",
    "\n",
    "year_df = df_with_sum.withColumn('month', month(df_with_sum['Date']))\n",
    "\n",
    "year_df.groupBy('month').max()['month', 'max(sum24H)'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the average charging volume for each calendar month?\n",
    "#### In other words, what is the average charging volume for the months of January, February, March, etc. in all years? You will be given the average of these months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|        avg(dfsum)|\n",
      "+-----+------------------+\n",
      "|    1|228.88515967741938|\n",
      "|    2|249.60757642857138|\n",
      "|    3|205.35038387096773|\n",
      "|    4|220.12105566666656|\n",
      "|    5| 239.5106480645161|\n",
      "|    6|216.61912066666665|\n",
      "|    7| 246.1196780645161|\n",
      "|    8|260.59578387096775|\n",
      "|    9|249.87301633333334|\n",
      "|   10| 256.9730729032258|\n",
      "|   11|249.54146933333337|\n",
      "|   12|244.11166344827592|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a new column Month from existing Date column\n",
    "month_df = year_df.withColumn('Month', month(df['Date']))\n",
    "\n",
    "#Group by month and take average of all other columns\n",
    "month_df = month_df.groupBy('Month').mean()\n",
    "\n",
    "#Sort by month\n",
    "month_df = month_df.orderBy('Month')\n",
    "\n",
    "#Display only month and avg(Close), the desired columns\n",
    "month_df['Month', 'avg(dfsum)'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
